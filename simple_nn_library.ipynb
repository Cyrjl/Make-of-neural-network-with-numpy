{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooking a simple neural network library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredients\n",
    "\n",
    "- `numpy`\n",
    "- [a loss function](#Loss-function)\n",
    "- [some layers](#Layers)\n",
    "- [a neural net](#Neural-network)\n",
    "- [an optimizer](#Optimizer)\n",
    "- [a batch data provider](#Batch-generator)\n",
    "- [a training routine](#Training)\n",
    "- \\+ [application exercises](#Application-exercise)\n",
    "\n",
    "Hopefully by the end of this tutorial you will have an understanding of the building blocks needed for training (deep) neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreword\n",
    "\n",
    "We will purely rely on numpy for this tutorial. Make sure to import it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Oriented Python\n",
    "\n",
    "Object-oriented Python, a.k.a _classes_, will be used intensively in this tutorial.  \n",
    "For those not familiar with Python classes, know that you will only be required to write some definitions and Python code **within the** class **methods** and **not** actually **write any class**.  \n",
    "\n",
    "If you want to know more about Python classes, here is a step by step [tutorial](https://aboucaud.github.io/slides/2016/python-classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type hints\n",
    "\n",
    "This notebook uses a feature from Python 3.5+ called ***type hints*** or ***type annotations*** (see [PEP 0526](https://www.python.org/dev/peps/pep-0526/)). This acts like optional static typing since Python will still run if the type does not match, but has two main advantages IMO:\n",
    "- make sure you understand what you're doing\n",
    "- act like documentation for an external user\n",
    "\n",
    "The types for the base Python objects (lists, dicts, iterables) can be found in the [`typing` library](https://docs.python.org/3/library/typing.html).\n",
    "For instance, here are all the needed imports for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (Dict, Tuple, Callable, \n",
    "                    Sequence, Iterator, NamedTuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any other Python object can serve as a type. We will use the `numpy.ndarray` in this tutorial to mock a tensor. We thus create a `Tensor` object to use as type hint throughout the code, and an object `Func` for a function that acts element-wise on a tensor and returns a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray as Tensor\n",
    "\n",
    "Func = Callable[[Tensor], Tensor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the type\n",
    "\n",
    "If type hints are optional for Python, they can still be used to actually check the consistency of the code. For this task, there is a module called [`mypy`](https://github.com/python/mypy) that can be used (not in this tutorial). \n",
    "\n",
    "Check out the [doc](http://mypy-lang.org/) if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "A loss function measures how good our predictions are, compared to the expected values. It is the cost function that needs to be minimised. The loss function must be differentiable, as its gradient is needed to adjust the parameters of the network through backpropagation.\n",
    "\n",
    "Below is generic loss class. It implements \n",
    "- `loss()` : **the loss** computated from the expected label and the predicted one,\n",
    "- `grad()` : **the gradient of the loss**, needed for the backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def grad(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice #1 - mean square error loss\n",
    "\n",
    "***5 min*** - *Implement the `MeanSquareError` class*\n",
    "\n",
    "In this exercise, `predicted` and `actual` are vectors (1-d numpy arrays).  \n",
    "You must implement both the loss and its derivative using these two vectors.\n",
    "\n",
    "For info, the mean square error loss function is defined as\n",
    "\n",
    "$$MSE(y_{true}, y_{pred}) = \\sum \\left(y_{pred} - y_{true}\\right) ^ 2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[6 6 6]\n"
     ]
    }
   ],
   "source": [
    "class MeanSquareError(Loss):\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        return np.sum((predicted - actual)**2)\n",
    "    \n",
    "    def grad(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        G = Tensor(shape=actual.shape, dtype=actual.dtype)\n",
    "        G.fill(-2*np.sum(predicted - actual))\n",
    "        return G\n",
    "    \n",
    "MSE = MeanSquareError()\n",
    "\n",
    "print(MSE.loss(np.array([0, 1, 2]), np.array([1, 2, 3])))\n",
    "print(MSE.grad(np.array([0, 1, 2]), np.array([1, 2, 3])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "\n",
    "### Neuron\n",
    "\n",
    "A neuron $\\mathscr{N}$ has multiple input values (vector $\\mathbf{x}$ of size $m$) and a single output $z$. Each neuron is characterised by its weights (vector $\\mathbf{w}$ of size $m$) and a constant bias $b$ to perform the linear operation\n",
    "\n",
    "$$\\begin{aligned} \n",
    "\\mathscr{N}_{\\mathbf{w}, b}(\\mathbf{x}) &= \\sum_i w_i.x_i + b \\\\\n",
    "                                   &= \\begin{bmatrix} w_{0} & \\cdots & w_{m}\\end{bmatrix} \\begin{bmatrix} x_0 \\\\ \\vdots \\\\ x_m \\end{bmatrix} + b\\\\\n",
    "                                   &= \\mathbf{w}^T\\mathbf{x} + b \\\\\n",
    "                                   &= z\n",
    "\\end{aligned}$$\n",
    "where $m$ is the input size, and output a single value $z$.\n",
    "\n",
    "\n",
    "### Linear layer\n",
    "\n",
    "A linear layer $\\mathscr{L}$ is a set of neurons, and can therefore be represented by a matrix of weights $\\mathbf{W}$ and a vector of constants $\\mathbf{b}$.  \n",
    "For a layer of $n$ neurons, the matrix $\\mathbf{W}$ is therefore $(m,n)$ and the vector $\\mathbf{b}$ is of size $n$.  \n",
    "\n",
    "The operation realized by the layer on the input vector $\\mathbf{x}$ of size $m$ is\n",
    "\n",
    "$$\\begin{aligned} \n",
    "\\mathscr{L}_{\\mathbf{W}, \\mathbf{b}}(\\mathbf{x}) \n",
    "% \\begin{bmatrix} y_0 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\n",
    "    &= \\begin{bmatrix} \\sum_i W_{i, 0}.x_i + b_0 \\\\ \\vdots \\\\  \\sum_i W_{i, n}.x_i + b_n \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} W_{0,0} & \\cdots & W_{m,0} \\\\ \\vdots & & \\vdots \\\\ W_{0,n} & \\cdots & W_{m,n} \\\\\\end{bmatrix} . \\begin{bmatrix} x_0 \\\\ \\vdots \\\\ x_m \\end{bmatrix} + \\begin{bmatrix} b_0 \\\\ \\vdots \\\\ b_n \\end{bmatrix} \\\\\n",
    "    &= \\mathbf{W}^T\\mathbf{x} + \\mathbf{b} \\\\\n",
    "    &= \\mathbf{z}\n",
    "\\end{aligned}$$\n",
    "\n",
    "which is a matrix multiplication and an addition, that produces an output vector $\\mathbf{z}$ of size $n$.\n",
    "\n",
    "### Activation layer\n",
    "\n",
    "After the layer forward pass, there might be an activation layer whose role is to break the linearity of the network. The so-called activation layer is thus a non-linear fonction $f$ acting on the output $\\mathbf{z}$ of the linear layer. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{a} &= f(\\mathbf{z}) \\\\\n",
    "           &= f(\\mathbf{W}^T\\mathbf{x} + \\mathbf{b})\n",
    "\\end{aligned}$$\n",
    "\n",
    "The activation layer conserves the shape.\n",
    "\n",
    "### Backpropagation (computing of the layer gradients)\n",
    "\n",
    "For the backward pass, each layer receives a gradient vector for the preceding layer.\n",
    "\n",
    "The ***chain rule*** connects the the loss $\\mathscr{C}$ (for cost) to the weights and biases of layer $i$ and yields the following relations  :\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\dfrac{\\partial \\mathscr{C}}{\\partial \\mathbf{W}^{i}} \n",
    "    &= \\dfrac{\\partial \\mathscr{C}}{\\partial \\mathbf{a}^{i}} \\cdot \\dfrac{\\partial \\mathbf{a}^{i}}{\\partial \\mathbf{z}^{i}} \\cdot \\dfrac{\\partial \\mathbf{z}^{i}}{\\partial \\mathbf{W}^{i}} \\\\\n",
    "    &= \\mathbf{\\nabla}\\mathscr{C}^{i} \\cdot f'(\\mathbf{z}^{i}) \\cdot \\mathbf{x}^{i} \\\\\n",
    "    &= \\mathbf{\\nabla_W^i}\n",
    "\\end{aligned}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\dfrac{\\partial \\mathscr{C}}{\\partial \\mathbf{b}^{i}} \n",
    "    &= \\dfrac{\\partial \\mathscr{C}}{\\partial \\mathbf{a}^{i}} \\cdot \\dfrac{\\partial \\mathbf{a}^{i}}{\\partial \\mathbf{z}^{i}} \\cdot \\dfrac{\\partial \\mathbf{z}^{i}}{\\partial \\mathbf{b}^{i}} \\\\\n",
    "    &= \\mathbf{\\nabla}\\mathscr{C}^{i} \\cdot f'(\\mathbf{z}^{i}) \\\\\n",
    "    &= \\mathbf{\\nabla_b^i}\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $\\mathbf{\\nabla}\\mathscr{C}^{i}$ is the gradient vector of the loss propagated at layer $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we define sequential neural nets, made of one or more layers.\n",
    "The layer will  pass its inputs forward\n",
    "and propagate gradients backward. \n",
    "\n",
    "For example, a neural net might look like `inputs -> Linear -> Tanh -> Linear -> output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base class for a layer has a dictionary to store parameters ($\\mathbf{W}$, $\\mathbf{b}$) and gradients ($\\mathbf{\\nabla_W}$, $\\mathbf{\\nabla_b}$) and implements a forward and a backward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self) -> None:\n",
    "        self.params: Dict[str, Tensor] = {}\n",
    "        self.grads: Dict[str, Tensor] = {}\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad: Tensor) -> Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice #2 - linear layer\n",
    "\n",
    "***10 - 15 min*** - *Implement the `forward` and `backward` methods of the linear layer.*\n",
    "\n",
    "The mathematical recap above is here to help you.  \n",
    "\n",
    "Be aware that neural networks are generally trained in batches (see [batch generator part](#Batch-generator)), essentially in order to \n",
    "- save some foward / backward computing steps\n",
    "- reduce the noise produced by extreme input vectors at the optimisation step.\n",
    "\n",
    "We therefore introduce the concept of ***batch_size***, which is the number of simultaneous trained inputs.\n",
    "For this reason, the input and output arrays of the layers are not actual vectors but **matrices** whose shape of one dimension is the ***batch_size***.\n",
    "\n",
    "Hints:\n",
    "- matrix products can be written either with `np.dot(m1, m2)` or `m1 @ m2` with recent Python versions (3.5+)\n",
    "- pay a specific attention to the shape of the input and output matrices for the matrix product\n",
    "- $\\mathbf{W}^T\\mathbf{x}$ is written `x @ W`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.05449003  -4.55194497  -1.76781244  -2.99323569]\n",
      " [  3.49950481 -11.94284484  -2.02032293  -6.43838512]]\n",
      "[[-122.32110229 -122.32110229 -122.32110229 -122.32110229]\n",
      " [-122.32110229 -122.32110229 -122.32110229 -122.32110229]]\n",
      "[[ -1.24202277 175.54386909 178.12747126]\n",
      " [ -1.24202277 175.54386909 178.12747126]]\n",
      "[[-2.39150019]\n",
      " [-2.65012385]\n",
      " [-0.63966774]\n",
      " [-0.8982914 ]]\n",
      "[[-17.15916637]\n",
      " [-17.15916637]\n",
      " [-17.15916637]\n",
      " [-17.15916637]]\n",
      "[[  4.43776651 -30.05998444]\n",
      " [  4.43776651 -30.05998444]\n",
      " [  4.43776651 -30.05998444]\n",
      " [  4.43776651 -30.05998444]]\n"
     ]
    }
   ],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"\n",
    "    Inputs are of size (batch_size, input_size)\n",
    "    Outputs are of size (batch_size, output_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, output_size: int) -> None:\n",
    "        # Inherit from base class Layer\n",
    "        super().__init__()\n",
    "        # Initialize the weights and bias with random values\n",
    "        self.params[\"w\"] = np.random.randn(input_size, output_size)\n",
    "        self.params[\"b\"] = np.random.randn(output_size)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        inputs shape is (batch_size, input_size)\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        # Compute here the feed forward pass\n",
    "        return inputs @ self.params[\"w\"] + self.params[\"b\"]    # (b,i) @ (i,o) + (1,o) = (b,o)\n",
    "     \n",
    "        \n",
    "    def backward(self, grad: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        grad shape is (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Compute here the gradient parameters for the layer\n",
    "        self.grads[\"w\"] = np.transpose(self.inputs) @ grad    # (i,b) @ (b,o) = (i,o)\n",
    "        self.grads[\"b\"] = grad                                # (b,o)\n",
    "        # Compute here the feed backward pass\n",
    "        return grad @ np.transpose(self.params[\"w\"])          # (b,o) @ (o,i) = (b,i) ?\n",
    "     \n",
    "L = Linear(3, 4)\n",
    "\n",
    "F = L.forward(Tensor(shape=(2,3), dtype=int, buffer=np.array([[1, 2, 3], [4, 5, 6]])))\n",
    "\n",
    "print(F)\n",
    "\n",
    "G = MSE.grad(np.array([[1, 2, 3, 4], [5, 6, 7, 8]]), F)\n",
    "\n",
    "print(G)\n",
    "\n",
    "print(L.backward(G))\n",
    "\n",
    "#########################\n",
    "\n",
    "B = Linear(2, 1)\n",
    "\n",
    "f = B.forward(np.array([[0, 0], [1, 0], [0, 1], [1, 1]]))\n",
    "\n",
    "print(f)\n",
    "\n",
    "p = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "g = MSE.grad(p, f)\n",
    "\n",
    "print(g)\n",
    "print(B.backward(g))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    \"\"\"\n",
    "    An activation layer just applies a function\n",
    "    elementwise to its inputs\n",
    "    \"\"\"\n",
    "    def __init__(self, f: Func, f_prime: Func) -> None:\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.f_prime = f_prime\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        self.inputs = inputs\n",
    "        return self.f(inputs)\n",
    "\n",
    "    def backward(self, grad: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        if y = f(x) and x = g(z)  # y = f(z) and z = g(x) --> y = f(x)\n",
    "        then dy/dz = f'(x) * g'(z)\n",
    "        \"\"\"\n",
    "        return self.f_prime(self.inputs) * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice #3 - tanh and sigmoid\n",
    "\n",
    "***5 min*** - *Implement the hyperbolic tangent and sigmoid layers and their derivatives.*\n",
    "\n",
    "Look for the definitions in the lecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tanh(x: Tensor) -> Tensor:\n",
    "    # Write here the tanh function\n",
    "    return math.tanh(x)\n",
    "\n",
    "def tanh_prime(x: Tensor) -> Tensor:\n",
    "    # Write here the derivative of the tanh\n",
    "    return 1 - (math.tanh(x))**2\n",
    "\n",
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__(tanh, tanh_prime)\n",
    "        \n",
    "\n",
    "def sigmoid(x: Tensor) -> Tensor:\n",
    "    # Write here the sigmoid function\n",
    "    return 1/(1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x: Tensor) -> Tensor:\n",
    "    # Write here the derivative of the sigmoid\n",
    "    return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__(sigmoid, sigmoid_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "A neural net is a collection of layers. It takes care of sequentially calling the layers `forward` and a `backward` methods in the right order.\n",
    "\n",
    "In addition, it implements a getter method `params_and_grads` that will be used by the optimizer to update the values of the weights and bias of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, layers: Sequence[Layer]) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        The forward pass takes the layers in order\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, grad: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        The backward pass is the other way around\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def params_and_grads(self) -> Iterator[Tuple[Tensor, Tensor]]:\n",
    "        for layer in self.layers:\n",
    "            for name, param in layer.params.items():\n",
    "                grad = layer.grads[name]\n",
    "                yield param, grad\n",
    "\n",
    "\n",
    "# A generator is an iterable (thing we can use \"for .. in ..\" on) but we can iterate it only once.\n",
    "# They forget the value they just calculated when calculating the new one\n",
    "# \"yield\" is used like \"return\", but returns a generator. When you call the function, the code\n",
    "# you have written in the function body does not run, the function only returns the generator object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "The role of the optimizer is to adjust the network parameters (weights and biases of the linear layers here) based on the gradients computed during backpropagation.\n",
    "\n",
    "The main attribute of an optimizer is the _learning rate_ (a.k.a. `lr`), which defines the size of the jump taken in the direction of the gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def step(self, net: NeuralNet) -> None:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice #4 - Stochastic Gradient Descent\n",
    "\n",
    "***5 min*** - write the optimizer step\n",
    "\n",
    "Here we have a very basic implementation of a _Stochastic Gradient Descent_ (a.k.a. `SGD`). \n",
    "\n",
    "The step that needs to be written iterates over the neural network layers and updates the layers parameters in the direction _opposite_ to the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr: float = 0.01) -> None:\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, net: NeuralNet) -> None:\n",
    "        for param, grad in net.params_and_grads(): \n",
    "            # Write here the parameters update\n",
    "            param = param - self.lr * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch generator\n",
    "\n",
    "It can be costly to compute the gradients and update the weights after every entry of the training dataset. In order to minimize such computational cost, the inputs of the network are traditionally fed in batches and the gradients are thus averages over those batches of data.\n",
    "\n",
    "A batch size of 32 is a default in multiple training sets. Some recent [study](https://arxiv.org/abs/1804.07612) claims this number is the perfect balance between computing efficiency and training stability.\n",
    "\n",
    "During an epoch the network will iterate over the whole dataset. Adding some shuffling in the process ensures the batches are not fed exactly in the same order at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = NamedTuple(\"Batch\", [(\"inputs\", Tensor), (\"targets\", Tensor)])\n",
    "\n",
    "\n",
    "class DataIterator:\n",
    "    def __call__(self, inputs: Tensor, targets: Tensor) -> Iterator[Batch]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "class BatchIterator(DataIterator):\n",
    "    def __init__(self, batch_size: int = 32, shuffle: bool = True) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __call__(self, inputs: Tensor, targets: Tensor) -> Iterator[Batch]:\n",
    "        starts = np.arange(0, len(inputs), self.batch_size)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(starts)\n",
    "\n",
    "        for start in starts:\n",
    "            end = start + self.batch_size\n",
    "            batch_inputs = inputs[start:end]\n",
    "            batch_targets = targets[start:end]\n",
    "            yield Batch(batch_inputs, batch_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training routine uses all objects defined above and executes actions **in the right order** to train the neural network.\n",
    "\n",
    "The dataset being usually small with respect to the number of free parameters of the neural net, going through the dataset multiple times during the training is a necessity. This corresponds to the number of epochs, which has to be specified.\n",
    "\n",
    "### Exercise #5 - build the training routine\n",
    "\n",
    "***10 min*** - write the sequential steps needed for training at each epoch\n",
    "\n",
    "_Hints_:\n",
    "- feed forward\n",
    "- compute the loss and the gradients\n",
    "- feed backwards\n",
    "- update the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net: NeuralNet, inputs: Tensor, targets: Tensor,\n",
    "          loss: Loss = MeanSquareError(), \n",
    "          optimizer: Optimizer = SGD(),\n",
    "          iterator: DataIterator = BatchIterator(),\n",
    "          num_epochs: int = 5000) -> None:\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in iterator(inputs, targets):\n",
    "            # Write here the various steps (in order) needed \n",
    "            # at each epoch\n",
    "            actual = net.forward(batch[0])\n",
    "            epoch_loss = loss.loss(batch[1], actual)\n",
    "            epoch_grad = loss.grad(batch[1], actual)\n",
    "            net.backward(epoch_grad)\n",
    "            optimizer.step(net)\n",
    "            \n",
    "        # Print status every 100 iterations\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application exercise\n",
    "\n",
    "Now that you have build your own neural network library, let's use it to solve a problem and then put it in application.\n",
    "\n",
    "### XOR\n",
    "\n",
    "Canonical problem in ML as there is not linear way to map the inputs to the output.\n",
    "\n",
    "```\n",
    "[0, 0] => 0  \n",
    "[0, 1] => 1  \n",
    "[1, 0] => 1  \n",
    "[1, 1] => 0  \n",
    "```\n",
    "\n",
    "Because of the extremely small size of the dataset, we will **forget** about the prescriptions on _training, validation and test sets_ for this example, which **you shouldn't do in practice**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "def print_xor_results(inputs: Tensor, targets: Tensor, predictions: Tensor) -> None:\n",
    "    print('\\nX => y => y_pred => round(y_pred)')\n",
    "    for x, y, z in zip(inputs, targets, predictions):\n",
    "        print(f'{x} => {y} => {z} => {z.round()}')\n",
    "        \n",
    "def train_xor(net: Optimizer, inputs: Tensor, targets: Tensor, epochs: int = 2000):\n",
    "    train(net, inputs, targets, num_epochs=epochs)\n",
    "    predictions = net.forward(inputs)\n",
    "    print_xor_results(inputs, targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an attempt at solving the XOR problem using a single linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.295804779168336 1\n",
      "100 8.295804779168336 1\n",
      "200 8.295804779168336 1\n",
      "300 8.295804779168336 1\n",
      "400 8.295804779168336 1\n",
      "500 8.295804779168336 1\n",
      "600 8.295804779168336 1\n",
      "700 8.295804779168336 1\n",
      "800 8.295804779168336 1\n",
      "900 8.295804779168336 1\n",
      "1000 8.295804779168336 1\n",
      "1100 8.295804779168336 1\n",
      "1200 8.295804779168336 1\n",
      "1300 8.295804779168336 1\n",
      "1400 8.295804779168336 1\n",
      "1500 8.295804779168336 1\n",
      "1600 8.295804779168336 1\n",
      "1700 8.295804779168336 1\n",
      "1800 8.295804779168336 1\n",
      "1900 8.295804779168336 1\n",
      "\n",
      "X => y => y_pred => round(y_pred)\n",
      "[0 0] => [0] => [-1.32447704] => [-1.]\n",
      "[1 0] => [1] => [-0.96051759] => [-1.]\n",
      "[0 1] => [1] => [-0.62212425] => [-1.]\n",
      "[1 1] => [0] => [-0.25816479] => [-0.]\n"
     ]
    }
   ],
   "source": [
    "net1 = NeuralNet([\n",
    "    Linear(input_size=2, output_size=1),\n",
    "])\n",
    "\n",
    "train_xor(net1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not work, as expected since XOR is a typical non-linear problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #6 - solve XOR with a Neural Net\n",
    "\n",
    "***5 min*** - Write a more advanced neural net (using additional linear and activation layers) until the predictions match the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'forward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m net2 \u001b[38;5;241m=\u001b[39m NeuralNet([\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Add the layers here\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m      4\u001b[0m ])\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_xor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m, in \u001b[0;36mtrain_xor\u001b[0;34m(net, inputs, targets, epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_xor\u001b[39m(net: Optimizer, inputs: Tensor, targets: Tensor, epochs: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mforward(inputs)\n\u001b[1;32m     12\u001b[0m     print_xor_results(inputs, targets, predictions)\n",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, inputs, targets, loss, optimizer, iterator, num_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iterator(inputs, targets):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Write here the various steps (in order) needed \u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# at each epoch\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     actual \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mloss(batch[\u001b[38;5;241m1\u001b[39m], actual)\n\u001b[1;32m     14\u001b[0m     epoch_grad \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mgrad(batch[\u001b[38;5;241m1\u001b[39m], actual)\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mNeuralNet.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mThe forward pass takes the layers in order\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 10\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m(inputs)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'forward'"
     ]
    }
   ],
   "source": [
    "net2 = NeuralNet([\n",
    "    # Add the layers here\n",
    "    ...\n",
    "])\n",
    "\n",
    "train_xor(net2, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #7 - write the same model using Keras\n",
    "\n",
    "***10 min*** - Based on the `Keras` examples given in the lecture, as well as the section on loss and optimizers, solve the XOR problem using `Keras` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down the keras model below\n",
    "#---------------------------------\n",
    "# Star by the necessary imports\n",
    "\n",
    "# Write the model architecture\n",
    "model = ...\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "\n",
    "# Train the model (no validation_split required here !!!!)\n",
    "\n",
    "#---------------------------------\n",
    "\n",
    "# Once trained, this will then predict the values (equivalent of `.forward()`)\n",
    "y_pred_keras = model.predict(X)\n",
    "\n",
    "# And print the results\n",
    "print_xor_results(X, y, y_pred_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to you now.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "The idea and the code for this tutorial have been for the most part inspired by the video \"Deep Learning Madness\" https://youtu.be/o64FV-ez6Gw by [Joel Grus](https://twitter.com/joelgrus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
